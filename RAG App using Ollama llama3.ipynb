{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f266f053-be61-421d-977c-d762179b2d6e",
      "metadata": {
        "id": "f266f053-be61-421d-977c-d762179b2d6e"
      },
      "outputs": [],
      "source": [
        "#pip install langchain-community==0.2.4 langchain==0.2.3 faiss-cpu==1.8.0 unstructured==0.14.5 unstructured[pdf]==0.14.5 transformers==4.41.2 sentence-transformers==3.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d701705-b332-4084-8643-65de741bd5eb",
      "metadata": {
        "id": "5d701705-b332-4084-8643-65de741bd5eb"
      },
      "source": [
        "**Importing the Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "24fae24c-433b-4860-badd-e0eb20759f4b",
      "metadata": {
        "id": "24fae24c-433b-4860-badd-e0eb20759f4b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a025e4d6-ce3b-4e5d-bbc7-fd32c1118d6b",
      "metadata": {
        "id": "a025e4d6-ce3b-4e5d-bbc7-fd32c1118d6b"
      },
      "outputs": [],
      "source": [
        "# loading the LLM\n",
        "llm = Ollama(\n",
        "    model=\"llama3:instruct\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09cb87e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/shobhandeb.paul/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'  # your path may be different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "378c5f69-192c-4db7-bc73-68d1fdbc1b2d",
      "metadata": {
        "id": "378c5f69-192c-4db7-bc73-68d1fdbc1b2d"
      },
      "outputs": [],
      "source": [
        "# loading the document\n",
        "# poppler_path = 'poppler-24.07.0/Library/bin'\n",
        "# loader = SimpleDirectoryReader(input_files=[r'C:\\Project\\GenAI\\NIPS-2017-attention-is-all-you-need-Paper.pdf']).load_data()\n",
        "# documents = loader\n",
        "\n",
        "file_path = r'C:\\Project\\GenAI\\NIPS-2017-attention-is-all-you-need-Paper.pdf'\n",
        "loader = PyPDFLoader(file_path)\n",
        "document = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunked_documents = text_splitter.split_documents(document)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e5d82c99-3c23-4870-b5ac-959cae3052e5",
      "metadata": {
        "id": "e5d82c99-3c23-4870-b5ac-959cae3052e5"
      },
      "outputs": [],
      "source": [
        "# # create document chunks\n",
        "# text_splitter = CharacterTextSplitter(separator=\"/n\",\n",
        "#                                       chunk_size=1000,\n",
        "#                                       chunk_overlap=200)\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     # Set a really small chunk size, just to show.\n",
        "#     chunk_size=100,\n",
        "#     chunk_overlap=20,\n",
        "#     length_function=len,\n",
        "#     is_separator_regex=False,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9592702b-23d2-46ab-a87d-b5a940be82f6",
      "metadata": {
        "id": "9592702b-23d2-46ab-a87d-b5a940be82f6"
      },
      "outputs": [],
      "source": [
        "# text_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "64d5e44f-631c-4d05-939d-a8aaf37bdd9e",
      "metadata": {
        "id": "64d5e44f-631c-4d05-939d-a8aaf37bdd9e",
        "outputId": "664357d5-a457-4e10-aec7-491761af0d6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shobhandeb.paul\\AppData\\Local\\Temp\\ipykernel_19016\\803604610.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "c:\\Project\\GenAI\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "c:\\Project\\GenAI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# loading the vector embedding model\n",
        "embeddings = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7cb5e2be-76e5-49f3-a50a-d336be3d30e0",
      "metadata": {
        "id": "7cb5e2be-76e5-49f3-a50a-d336be3d30e0"
      },
      "outputs": [],
      "source": [
        "# knowledge_base = FAISS.from_documents(text_chunks, embeddings)\n",
        "knowledge_base = FAISS.from_documents(chunked_documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ab0d7fda-3c61-49db-b211-275c083175b7",
      "metadata": {
        "id": "ab0d7fda-3c61-49db-b211-275c083175b7"
      },
      "outputs": [],
      "source": [
        "# retrieval QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=knowledge_base.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "154f2f47-1bc8-4d48-85f6-0775e02f00cc",
      "metadata": {
        "id": "154f2f47-1bc8-4d48-85f6-0775e02f00cc",
        "outputId": "df0127ec-d115-4f04-9cf5-1e2f1a58fe92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This document appears to be an academic paper or research article on the Transformer model, specifically discussing its architecture and training regime for machine translation tasks.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is this document about?\"\n",
        "response = qa_chain.invoke({\"query\": question})\n",
        "print(response[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "939c4925-440f-458e-85be-4b8c772ba0d4",
      "metadata": {
        "id": "939c4925-440f-458e-85be-4b8c772ba0d4",
        "outputId": "99db6dec-1d25-48e9-a420-b0825fce4c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The architecture discussed in the model is a Transformer model that follows an overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder is composed of a stack of 6 identical layers, each with two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The decoder is also composed of a stack of 6 identical layers, with an additional third sub-layer that performs multi-head attention over the output of the encoder stack.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the architecture discussed in the model?\"\n",
        "response = qa_chain.invoke({\"query\": question})\n",
        "print(response[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5cf37f5-0b6e-42ef-b7e7-a37e2c630aa3",
      "metadata": {
        "id": "c5cf37f5-0b6e-42ef-b7e7-a37e2c630aa3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
